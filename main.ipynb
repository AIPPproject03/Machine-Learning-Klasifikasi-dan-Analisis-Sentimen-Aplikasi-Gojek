{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ed106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n",
      "Dataset shape: (225002, 6)\n",
      "\n",
      "Column information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 225002 entries, 0 to 225001\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   userName       225002 non-null  object\n",
      " 1   content        225000 non-null  object\n",
      " 2   score          225002 non-null  int64 \n",
      " 3   at             225002 non-null  object\n",
      " 4   appVersion     225002 non-null  object\n",
      " 5   sentimenLabel  225002 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 10.3+ MB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      "userName         0\n",
      "content          2\n",
      "score            0\n",
      "at               0\n",
      "appVersion       0\n",
      "sentimenLabel    0\n",
      "dtype: int64\n",
      "\n",
      "Sample data:\n",
      "                  userName                                            content  score                   at appVersion sentimenLabel\n",
      "0                Yuga Edit                            akun gopay saya di blok      1  2022-01-21 10:52:12      4.9.3       negatif\n",
      "1                 ff burik  Lambat sekali sekarang ini bosssku apk gojek g...      3  2021-11-30 15:40:38      4.9.3        netral\n",
      "2  Anisa Suci Rahmayuliani  Kenapa sih dari kemarin sy buka aplikasi gojek...      4  2021-11-29 22:58:12      4.9.3       positif\n",
      "3             naoki yakuza  Baru download gojek dan hape baru trus ditop u...      1  2022-09-03 15:21:17      4.9.3       negatif\n",
      "4            Trio Sugianto                                             Mantap      5  2022-01-15 10:05:27      4.9.3       positif\n",
      "\n",
      "Sentiment distribution:\n",
      "sentimenLabel\n",
      "positif    161371\n",
      "negatif     54171\n",
      "netral       9460\n",
      "Name: count, dtype: int64\n",
      "Training set size: 180001\n",
      "Testing set size: 45001\n",
      "\n",
      "Applying TF-IDF Vectorization...\n",
      "TF-IDF feature matrix shape: (180001, 44775)\n",
      "Number of features: 44775\n",
      "\n",
      "Applying Bag of Words Vectorization...\n",
      "Bag of Words feature matrix shape: (180001, 44775)\n",
      "Number of features: 44775\n",
      "\n",
      "Training FastText Word Embeddings...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    177\u001b[39m tokenized_sentences = [tokenize_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m X_train]\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Train FastText model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m fasttext_model = \u001b[43mFastText\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenized_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Embedding dimension\u001b[39;49;00m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Context window size\u001b[39;49;00m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Minimum word count\u001b[39;49;00m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Number of threads\u001b[39;49;00m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43msg\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# Skip-gram model (1) vs CBOW (0)\u001b[39;49;00m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Number of training epochs\u001b[39;49;00m\n\u001b[32m    188\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFastText model trained successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Function to create document vectors from FastText word embeddings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\fasttext.py:435\u001b[39m, in \u001b[36mFastText.__init__\u001b[39m\u001b[34m(self, sentences, corpus_file, sg, hs, vector_size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, max_final_vocab, shrink_windows)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28mself\u001b[39m.wv.vectors_vocab_lockf = ones(\u001b[32m1\u001b[39m, dtype=REAL)\n\u001b[32m    433\u001b[39m \u001b[38;5;28mself\u001b[39m.wv.vectors_ngrams_lockf = ones(\u001b[32m1\u001b[39m, dtype=REAL)\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFastText\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msg\u001b[49m\u001b[43m=\u001b[49m\u001b[43msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_vocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_final_vocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_final_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_vocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43msorted_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnull_word\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnull_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns_exponent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mns_exponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhashfxn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhashfxn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbow_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcbow_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshrink_windows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshrink_windows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:429\u001b[39m, in \u001b[36mWord2Vec.__init__\u001b[39m\u001b[34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_corpus_sanity(corpus_iterable=corpus_iterable, corpus_file=corpus_file, passes=(epochs + \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28mself\u001b[39m.train(\n\u001b[32m    431\u001b[39m         corpus_iterable=corpus_iterable, corpus_file=corpus_file, total_examples=\u001b[38;5;28mself\u001b[39m.corpus_count,\n\u001b[32m    432\u001b[39m         total_words=\u001b[38;5;28mself\u001b[39m.corpus_total_words, epochs=\u001b[38;5;28mself\u001b[39m.epochs, start_alpha=\u001b[38;5;28mself\u001b[39m.alpha,\n\u001b[32m    433\u001b[39m         end_alpha=\u001b[38;5;28mself\u001b[39m.min_alpha, compute_loss=\u001b[38;5;28mself\u001b[39m.compute_loss, callbacks=callbacks)\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:497\u001b[39m, in \u001b[36mWord2Vec.build_vocab\u001b[39m\u001b[34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[39m\n\u001b[32m    495\u001b[39m report_values = \u001b[38;5;28mself\u001b[39m.prepare_vocab(update=update, keep_raw_vocab=keep_raw_vocab, trim_rule=trim_rule, **kwargs)\n\u001b[32m    496\u001b[39m report_values[\u001b[33m'\u001b[39m\u001b[33mmemory\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.estimate_memory(vocab_size=report_values[\u001b[33m'\u001b[39m\u001b[33mnum_retained_words\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28mself\u001b[39m.add_lifecycle_event(\u001b[33m\"\u001b[39m\u001b[33mbuild_vocab\u001b[39m\u001b[33m\"\u001b[39m, update=update, trim_rule=\u001b[38;5;28mstr\u001b[39m(trim_rule))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:853\u001b[39m, in \u001b[36mWord2Vec.prepare_weights\u001b[39m\u001b[34m(self, update)\u001b[39m\n\u001b[32m    851\u001b[39m \u001b[38;5;66;03m# set initial input/projection and hidden weights\u001b[39;00m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m update:\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    855\u001b[39m     \u001b[38;5;28mself\u001b[39m.update_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:864\u001b[39m, in \u001b[36mWord2Vec.init_weights\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    862\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\"\"\"\u001b[39;00m\n\u001b[32m    863\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mresetting layer weights\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hs:\n\u001b[32m    867\u001b[39m     \u001b[38;5;28mself\u001b[39m.syn1 = np.zeros((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.wv), \u001b[38;5;28mself\u001b[39m.layer1_size), dtype=REAL)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\fasttext.py:1158\u001b[39m, in \u001b[36mFastTextKeyedVectors.resize_vectors\u001b[39m\u001b[34m(self, seed)\u001b[39m\n\u001b[32m   1155\u001b[39m ngrams_shape = (\u001b[38;5;28mself\u001b[39m.bucket, \u001b[38;5;28mself\u001b[39m.vector_size)\n\u001b[32m   1156\u001b[39m \u001b[38;5;28mself\u001b[39m.vectors_ngrams = prep_vectors(ngrams_shape, prior_vectors=\u001b[38;5;28mself\u001b[39m.vectors_ngrams, seed=seed + \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mallocate_vecattrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[38;5;28mself\u001b[39m.norms = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28mself\u001b[39m.recalc_char_ngram_buckets()  \u001b[38;5;66;03m# ensure new words have precalc buckets\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:299\u001b[39m, in \u001b[36mKeyedVectors.allocate_vecattrs\u001b[39m\u001b[34m(self, attrs, types)\u001b[39m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msample_int\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.expandos:\n\u001b[32m    297\u001b[39m         \u001b[38;5;28mself\u001b[39m.expandos[\u001b[33m'\u001b[39m\u001b[33msample_int\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.expandos[\u001b[33m'\u001b[39m\u001b[33msample_int\u001b[39m\u001b[33m'\u001b[39m].astype(np.uint32)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mallocate_vecattrs\u001b[39m(\u001b[38;5;28mself\u001b[39m, attrs=\u001b[38;5;28;01mNone\u001b[39;00m, types=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    300\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\u001b[39;00m\n\u001b[32m    301\u001b[39m \n\u001b[32m    302\u001b[39m \u001b[33;03m    The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m \n\u001b[32m    306\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    307\u001b[39m     \u001b[38;5;66;03m# with no arguments, adjust lengths of existing vecattr arrays to match length of index_to_key\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from gensim.models import FastText\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "\n",
    "# Create directories for models if they don't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('data/GojekAppReview_Processed.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nColumn information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"\\nSentiment distribution:\")\n",
    "sentiment_counts = df['sentimenLabel'].value_counts()\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='sentimenLabel', data=df, palette='viridis')\n",
    "plt.title('Sentiment Distribution', fontsize=16)\n",
    "plt.xlabel('Sentiment', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Visualize rating distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='score', data=df, palette='plasma')\n",
    "plt.title('Rating Distribution', fontsize=16)\n",
    "plt.xlabel('Rating', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Relationship between score and sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "cross_tab = pd.crosstab(df['score'], df['sentimenLabel'])\n",
    "cross_tab.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "plt.title('Relationship between Score and Sentiment', fontsize=16)\n",
    "plt.xlabel('Score', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Sentiment', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Content length analysis\n",
    "df['content_length'] = df['content'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='sentimenLabel', y='content_length', data=df, palette='viridis')\n",
    "plt.title('Review Length by Sentiment', fontsize=16)\n",
    "plt.xlabel('Sentiment', fontsize=14)\n",
    "plt.ylabel('Review Length (characters)', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Create a copy of preprocessed data for model training\n",
    "model_df = df.copy()\n",
    "\n",
    "# Prepare the data for modeling with proper handling of NaN values\n",
    "X = model_df['content'].fillna('')  # Replace NaN values with empty strings\n",
    "y = model_df['sentimenLabel']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "# Feature Engineering Method 1: TF-IDF Vectorization\n",
    "print(\"\\nApplying TF-IDF Vectorization...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=5,                # Minimum document frequency\n",
    "    max_df=0.8,              # Maximum document frequency\n",
    "    sublinear_tf=True,       # Apply sublinear tf scaling (1 + log(tf))\n",
    "    use_idf=True,            # Apply IDF weighting\n",
    "    ngram_range=(1, 2)       # Use unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Number of features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Feature Engineering Method 2: Bag of Words (CountVectorizer)\n",
    "print(\"\\nApplying Bag of Words Vectorization...\")\n",
    "count_vectorizer = CountVectorizer(\n",
    "    min_df=5,                # Minimum document frequency\n",
    "    max_df=0.8,              # Maximum document frequency\n",
    "    ngram_range=(1, 2)       # Use unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = count_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Bag of Words feature matrix shape: {X_train_bow.shape}\")\n",
    "print(f\"Number of features: {len(count_vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Feature Engineering Method 3: FastText Word Embeddings\n",
    "print(\"\\nTraining FastText Word Embeddings...\")\n",
    "\n",
    "# Tokenize text for FastText\n",
    "def tokenize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize\n",
    "    return text.split()\n",
    "\n",
    "# Prepare sentences for FastText training\n",
    "tokenized_sentences = [tokenize_text(text) for text in X_train]\n",
    "\n",
    "# Train FastText model\n",
    "fasttext_model = FastText(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,         # Embedding dimension\n",
    "    window=5,                # Context window size\n",
    "    min_count=5,             # Minimum word count\n",
    "    workers=4,               # Number of threads\n",
    "    sg=1,                    # Skip-gram model (1) vs CBOW (0)\n",
    "    epochs=10                # Number of training epochs\n",
    ")\n",
    "\n",
    "print(\"FastText model trained successfully\")\n",
    "\n",
    "# Function to create document vectors from FastText word embeddings\n",
    "def document_vector(text, model, vector_size=100):\n",
    "    words = tokenize_text(text)\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Create document vectors for training and testing data\n",
    "X_train_fasttext = np.array([document_vector(text, fasttext_model) for text in X_train])\n",
    "X_test_fasttext = np.array([document_vector(text, fasttext_model) for text in X_test])\n",
    "\n",
    "print(f\"FastText document vectors shape: {X_train_fasttext.shape}\")\n",
    "\n",
    "# Prepare data for deep learning model\n",
    "print(\"\\nPreparing data for deep learning model...\")\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences for consistent input length\n",
    "max_length = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "print(f\"Padded sequence shape: {X_train_pad.shape}\")\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['negatif', 'netral', 'positif'],\n",
    "                yticklabels=['negatif', 'netral', 'positif'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=16)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Function for cross-validation\n",
    "def cross_validate_model(model, X, y, cv=5, model_name=\"\"):\n",
    "    print(f\"\\nPerforming {cv}-fold cross-validation for {model_name}...\")\n",
    "    \n",
    "    # Define cross-validation strategy\n",
    "    kfold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_accuracy = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
    "    cv_precision = cross_val_score(model, X, y, cv=kfold, scoring='precision_weighted')\n",
    "    cv_recall = cross_val_score(model, X, y, cv=kfold, scoring='recall_weighted')\n",
    "    cv_f1 = cross_val_score(model, X, y, cv=kfold, scoring='f1_weighted')\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Cross-Validation Results for {model_name}:\")\n",
    "    print(f\"Accuracy: {cv_accuracy.mean():.4f} (±{cv_accuracy.std():.4f})\")\n",
    "    print(f\"Precision: {cv_precision.mean():.4f} (±{cv_precision.std():.4f})\")\n",
    "    print(f\"Recall: {cv_recall.mean():.4f} (±{cv_recall.std():.4f})\")\n",
    "    print(f\"F1 Score: {cv_f1.mean():.4f} (±{cv_f1.std():.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': cv_accuracy,\n",
    "        'precision': cv_precision,\n",
    "        'recall': cv_recall,\n",
    "        'f1': cv_f1\n",
    "    }\n",
    "\n",
    "# Model 1: Multinomial Naive Bayes with TF-IDF features\n",
    "print(\"\\nTraining Multinomial Naive Bayes model with TF-IDF features...\")\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_nb_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "nb_tfidf_metrics = evaluate_model(y_test, y_pred_nb_tfidf, \"Naive Bayes (TF-IDF)\")\n",
    "\n",
    "# Cross-validation\n",
    "nb_tfidf_cv = cross_validate_model(MultinomialNB(), X_train_tfidf, y_train, cv=5, model_name=\"Naive Bayes (TF-IDF)\")\n",
    "\n",
    "# Model 2: Linear SVM with TF-IDF features\n",
    "print(\"\\nTraining Linear SVM model with TF-IDF features...\")\n",
    "svm_tfidf = LinearSVC(random_state=42)\n",
    "svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "svm_tfidf_metrics = evaluate_model(y_test, y_pred_svm_tfidf, \"Linear SVM (TF-IDF)\")\n",
    "\n",
    "# Cross-validation\n",
    "svm_tfidf_cv = cross_validate_model(LinearSVC(random_state=42), X_train_tfidf, y_train, cv=5, model_name=\"Linear SVM (TF-IDF)\")\n",
    "\n",
    "# Model 3: Random Forest with FastText features\n",
    "print(\"\\nTraining Random Forest model with FastText features...\")\n",
    "rf_fasttext = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_fasttext.fit(X_train_fasttext, y_train)\n",
    "y_pred_rf_fasttext = rf_fasttext.predict(X_test_fasttext)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_fasttext_metrics = evaluate_model(y_test, y_pred_rf_fasttext, \"Random Forest (FastText)\")\n",
    "\n",
    "# Cross-validation\n",
    "rf_fasttext_cv = cross_validate_model(RandomForestClassifier(n_estimators=100, random_state=42), \n",
    "                                     X_train_fasttext, y_train, cv=5, \n",
    "                                     model_name=\"Random Forest (FastText)\")\n",
    "\n",
    "# Compare traditional ML models\n",
    "models = ['Naive Bayes (TF-IDF)', 'Linear SVM (TF-IDF)', 'Random Forest (FastText)']\n",
    "accuracies = [nb_tfidf_metrics[0], svm_tfidf_metrics[0], rf_fasttext_metrics[0]]\n",
    "precisions = [nb_tfidf_metrics[1], svm_tfidf_metrics[1], rf_fasttext_metrics[1]]\n",
    "recalls = [nb_tfidf_metrics[2], svm_tfidf_metrics[2], rf_fasttext_metrics[2]]\n",
    "f1_scores = [nb_tfidf_metrics[3], svm_tfidf_metrics[3], rf_fasttext_metrics[3]]\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "x = np.arange(len(models))\n",
    "width = 0.2\n",
    "plt.bar(x - width*1.5, accuracies, width, label='Accuracy', color='skyblue')\n",
    "plt.bar(x - width/2, precisions, width, label='Precision', color='lightgreen')\n",
    "plt.bar(x + width/2, recalls, width, label='Recall', color='salmon')\n",
    "plt.bar(x + width*1.5, f1_scores, width, label='F1 Score', color='purple')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xlabel('Models', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.title('Traditional ML Models Performance Comparison', fontsize=16)\n",
    "plt.xticks(x, models, rotation=45, ha='right')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the best traditional model\n",
    "best_traditional_model = svm_tfidf  # Assuming SVM performs best, adjust based on actual results\n",
    "with open('models/svm_tfidf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_traditional_model, f)\n",
    "with open('models/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "print(\"Best traditional model saved!\")\n",
    "\n",
    "# Prepare label encoding for deep learning\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Get number of classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Create embeddings matrix for pre-trained weights\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Fill embedding matrix with FastText vectors\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in fasttext_model.wv:\n",
    "        embedding_matrix[i] = fasttext_model.wv[word]\n",
    "\n",
    "# Build Bidirectional LSTM model with FastText embeddings\n",
    "print(\"\\nBuilding Bidirectional LSTM model...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, \n",
    "                   weights=[embedding_matrix], \n",
    "                   input_length=max_length, \n",
    "                   trainable=False))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train model with early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(X_train_pad, y_train_encoded,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test_encoded)\n",
    "print(f\"\\nBiLSTM Model Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred_encoded = np.argmax(y_pred_probs, axis=1)\n",
    "y_pred_dl = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Evaluate the deep learning model\n",
    "dl_metrics = evaluate_model(y_test, y_pred_dl, \"BiLSTM with FastText Embeddings\")\n",
    "\n",
    "# Save the deep learning model\n",
    "model.save('models/bilstm_model.keras')\n",
    "with open('models/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "with open('models/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"Deep learning model saved!\")\n",
    "\n",
    "# Word cloud visualization for each sentiment\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        background_color='white', \n",
    "        colormap='viridis',\n",
    "        max_words=100,\n",
    "        contour_width=3,\n",
    "        contour_color='steelblue'\n",
    "    ).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create wordclouds for each sentiment\n",
    "for sentiment in ['positif', 'netral', 'negatif']:\n",
    "    # Filter out NaN values and convert all values to strings\n",
    "    filtered_content = df[df['sentimenLabel'] == sentiment]['content'].dropna().astype(str)\n",
    "    sentiment_texts = ' '.join(filtered_content)\n",
    "    generate_wordcloud(sentiment_texts, f'Word Cloud - {sentiment.capitalize()} Reviews')\n",
    "\n",
    "# N-gram analysis\n",
    "def get_top_n_words(corpus, n_gram_range, n=10):\n",
    "    vec = CountVectorizer(ngram_range=n_gram_range, stop_words=stopwords.words('indonesian')).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Get top unigrams for each sentiment\n",
    "for sentiment in ['positif', 'netral', 'negatif']:\n",
    "    # Filter out NaN values before processing\n",
    "    sentiment_texts = df[df['sentimenLabel'] == sentiment]['content'].dropna().astype(str)\n",
    "    top_words = get_top_n_words(sentiment_texts, (1, 1), 15)\n",
    "    \n",
    "    words = [word[0] for word in top_words]\n",
    "    counts = [word[1] for word in top_words]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(words, counts, color='skyblue')\n",
    "    plt.title(f'Top 15 Words in {sentiment.capitalize()} Reviews', fontsize=16)\n",
    "    plt.xlabel('Words', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Length analysis by sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "for sentiment in ['positif', 'netral', 'negatif']:\n",
    "    sns.kdeplot(df[df['sentimenLabel'] == sentiment]['content_length'], label=sentiment)\n",
    "plt.title('Review Length Distribution by Sentiment', fontsize=16)\n",
    "plt.xlabel('Review Length (characters)', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.legend(title='Sentiment')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Time-based analysis\n",
    "df['date'] = pd.to_datetime(df['at']).dt.date\n",
    "sentiment_by_date = df.groupby(['date', 'sentimenLabel']).size().unstack().fillna(0)\n",
    "\n",
    "# Calculate sentiment ratio over time\n",
    "sentiment_ratio = sentiment_by_date.div(sentiment_by_date.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot sentiment trends over time\n",
    "plt.figure(figsize=(14, 7))\n",
    "sentiment_ratio.plot(kind='line', linewidth=2)\n",
    "plt.title('Sentiment Ratio Trends Over Time', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Proportion of Sentiments', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
